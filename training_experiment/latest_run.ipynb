{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66a92319135148d",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "This Python script implements and evaluates a Transformer-based language model incorporating Mixture-of-Experts (MoE) layers. The core goal is to investigate the impact of different routing strategies within the MoE layers on model performance and behavior. It leverages the torch library for model building and training, pandas for results management, and potentially the entmax library for implementing α-entmax routing functions.\n",
    "\n",
    "## Modules\n",
    "\n",
    "1. Model Architecture (SparseMoELanguageModel):\n",
    "    - Builds upon a standard Transformer decoder architecture.\n",
    "    - Replaces the standard position-wise Feed-Forward Networks (FFNs) in some or all Transformer blocks with SparseMoE layers.\n",
    "2. Routing (AlphaEntmaxRouter):\n",
    "    - Implements the gating mechanism responsible for calculating expert probabilities for each input token.\n",
    "    - Uses `entmax_bisect` for `alpha` > 1.0 to compute the α-entmax transformation, which includes Sparsemax (`alpha=2.0`) as a special case.\n",
    "        - Applies temperature scaling to the logits before the softmax/entmax calculation.\n",
    "3. Expert Combination (SparseMoE):\n",
    "    - Calculates routing probabilities using the AlphaEntmaxRouter.\n",
    "    - Determines which experts are \"active\" based on probabilities exceeding a small threshold (`activation_threshold`).\n",
    "    - Computes the final output by taking a weighted sum of the outputs from the active experts.\n",
    "4. Training and Evaluation:\n",
    "    - Uses standard cross-entropy loss for language modeling.\n",
    "    - periodically evaluate the model on training and validation splits, calculating loss and various MoE-specific metrics.\n",
    "5. Metrics Calculation (`SparseMoE.calculate_metrics`):\n",
    "    - Computes several metrics based on the router's output probabilities during evaluation:\n",
    "    - Expert Utilization: Fraction of experts used.\n",
    "    - Load Imbalance Ratio: Ratio of max-to-mean token load per expert.\n",
    "    - Routing Concentration: Average maximum probability per token.\n",
    "    - Expert Load Variation (CV): Coefficient of variation for token load.\n",
    "    - Routing Probability CV: Coefficient of variation for average expert probability.\n",
    "6. Experiment Structure:\n",
    "    - Hyperparameter Sweep: Systematically trains models with different combinations of `router_alpha` and `router_temperature` using a smaller configuration (`config_sweep`) on a 90% split of the dataset (Tiny Shakespeare). Crucially, it runs multiple trials (5 seeds) for each combination, re-initializing the model and optimizer for each seed to ensure robustness against random variations. Results (losses and metrics per iteration) for all runs are logged to a DataFrame.\n",
    "    - Sweep Aggregation: After the sweep, it calculates and reports the mean and standard deviation of the final validation loss and other metrics for each (`alpha`, `temperature`) pair across the different seeds. It identifies the best parameters based on the lowest average validation loss.\n",
    "    - Full Dataset Run: Performs a longer training run using a larger configuration (`config_full`) and the best hyperparameters found from the sweep (or defaults if the sweep failed). This run uses the full dataset (re-split 90/10 for validation) and also runs across multiple seeds (`num_seeds_full = 3`) for robustness. Results are logged and aggregated similarly.\n",
    "7. Logging: All intermediate and final results from both the sweep and the full run are saved to CSV files (`sweep_results_all_seeds`.csv, `sweep_results_aggregated`.csv, `full_run_results_all_seeds.csv`, `full_run_results_aggregated.csv`) for analysis, suitable for remote execution environments.\n",
    "\n",
    "\n",
    "### METRICS (details)\n",
    "Let:\n",
    "- $N$ be the total number of experts.\n",
    "- $T_{\\text{batch}}$ be the total number of tokens evaluated (summed over eval_iters batches).\n",
    "- $p_i^t$ be the routing probability assigned by the router to expert $i$ for token $t$.\n",
    "- $\\tau$ be the activation_threshold (e.g., 1e-9).\n",
    "- $I(t, i)=\\mathbf{1}\\left[p_i^t>\\tau\\right]$ be the indicator function, equaling 1 if expert $i$ is considered active for token $t$, and 0 otherwise.\n",
    "- $T P E_i=\\sum_{t=1}^{T_{\\text{batch}}} I(t, i)$ be the total number of tokens considered processed by expert $i$ during evaluation.\n",
    "\n",
    "1. Expert Utilization\n",
    "$$\n",
    "\\text{Utilization}=\\frac{1}{N} \\sum_{i=1}^N \\mathbf{1}\\left[TPE_i>0\\right]\n",
    "$$\n",
    "The fraction of experts that processed at least one token (i.e., had a routing probability greater than $\\tau$ for at least one token) during the evaluation period.\n",
    "Measures how many experts are actually engaged by the router. A value of 1.0 indicates all experts were used; a value less than 1.0 indicates some experts remained completely idle.\n",
    "2. Load Imbalance Ratio\n",
    "Let $\\mu_{TPE}=\\frac{1}{N} \\sum_{i=1}^N T P E_i$ (mean tokens per expert).\n",
    "$$\n",
    "\\text{Load Imbalance Ratio}= \\begin{cases}\\frac{\\max _i\\left(TPE_i\\right)}{\\mu_{TPE}+\\epsilon} & \\text{if} \\mu_{TPE}>0 \\\\ 1.0 & \\text{if} \\mu_{TPE}=0\\end{cases}\n",
    "$$\n",
    "(where $\\epsilon$ is a small constant like $1 \\mathrm{e}-9$ to prevent division by zero).\n",
    "The ratio of the maximum number of tokens processed by any single expert to the average number of tokens processed per expert.\n",
    "Quantifies the unevenness of the token load distribution based on activation counts. A value of 1.0 signifies perfect balance (all active experts processed the same number of tokens). Values greater than 1 indicate imbalance, with higher values signifying that the busiest expert handled a much larger share of tokens than the average.\n",
    "3. Load Imbalance Ratio\n",
    "Let $\\mu_{TPE}=\\frac{1}{N} \\sum_{i=1}^N T P E_i$ (mean tokens per expert).\n",
    "$$\n",
    "\\text{Load Imbalance Ratio}= \\begin{cases}\\frac{\\max _i\\left(TPE_i\\right)}{\\mu_{TPE}+\\epsilon} & \\text{if} \\mu_{TPE}>0 \\\\ 1.0 & \\text{if} \\mu_{TPE}=0\\end{cases}\n",
    "$$\n",
    "(where $\\epsilon$ is a small constant like $1 \\mathrm{e}-9$ to prevent division by zero).\n",
    "The ratio of the maximum number of tokens processed by any single expert to the average number of tokens processed per expert.\n",
    "Quantifies the unevenness of the token load distribution based on activation counts. A value of 1.0 signifies perfect balance (all active experts processed the same number of tokens). Values greater than 1 indicate imbalance, with higher values signifying that the busiest expert handled a much larger share of tokens than the average.\n",
    "4. Routing Concentration\n",
    "$$\n",
    "\\text{Routing Concentration}=\\frac{1}{T_{\\text{batch}}} \\sum_{t=1}^{T_{\\text{batch}}}\\left(\\max _{i \\in\\{1 \\ldots N\\}} p_i^t\\right)\n",
    "$$\n",
    "The average, over all evaluated tokens, of the highest routing probability assigned to any single expert for that token.\n",
    "Measures the average \"confidence\" or \"peakiness\" of the router's probability distribution per token. Values range from $1 / N$ (for perfectly uniform probabilities) to 1.0 (when one expert consistently receives probability 1). Higher values correlate with sparser routing decisions.\n",
    "5. Expert Load Variation (CV)\n",
    "Let $\\mu_{TPE}=\\frac{1}{N} \\sum_{i=1}^N T P E_i$. Let $\\sigma_{TPE}=\\sqrt{\\frac{1}{N} \\sum_{i=1}^N\\left(TPE_i-\\mu_{TPE}\\right)^2}$ (Population standard deviation of TPE).\n",
    "$$\n",
    "\\text{Expert Load Variation}(\\mathrm{CV})= \\begin{cases}\\frac{\\sigma_{TPE}}{\\mu_{TPE}+\\epsilon} & \\text{if} \\mu_{TPE}>0 \\text{and} N>1 \\\\ 0.0 & \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "The coefficient of variation (standard deviation divided by the mean) of the number of tokens processed per expert ( $T P E_i$ ).\n",
    "Measures the relative variability in the workload across experts, normalized by the average workload. A value of $O$ indicates perfect balance in token counts. Higher values indicate greater relative differences in the number of tokens processed by each expert.\n",
    "6. Routing Probability CV\n",
    "Let $\\bar{p}_i=\\frac{1}{T_{\\text{batch}}} \\sum_{t=1}^{T_{\\text{batch}}} p_i^t$ (average probability assigned to expert $i$ ).\n",
    "\n",
    "Let $\\mu_{\\bar{p}}=\\frac{1}{N} \\sum_{i=1}^N \\bar{p}_i=\\frac{1}{N}$. Let $\\sigma_{\\bar{p}}=\\sqrt{\\frac{1}{N} \\sum_{i=1}^N\\left(\\bar{p}_i-\\mu_{\\bar{p}}\\right)^2}$ (Population standard deviation of $\\bar{p}_i$ ).\n",
    "$$\n",
    "\\text{Routing Probability CV}= \\begin{cases}\\frac{\\sigma_{\\bar{p}}}{\\mu_{\\bar{p}+\\epsilon}} & \\text{if} \\mu_{\\bar{p}}>0 \\text{and} N>1 \\\\ 0.0 & \\text{otherwise}\\end{cases}\n",
    "$$\n",
    "The coefficient of variation (standard deviation divided by the mean) of the average routing probabilities assigned to each expert over the evaluated tokens.\n",
    "Higher values indicate greater relative differences in the average routing probabilities assigned to each expert. A value of 0.0 indicates perfect balance in routing probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e025619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e10db440717a546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported entmax_bisect.\n",
      "Using device: cuda\n",
      "Successfully loaded input.txt\n",
      "Dataset Stats: Vocab size: 65, Total tokens: 1115394\n",
      "Sweep Data Split: Train tokens: 1003854, Valid tokens: 111540\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=0.8, rc={\"figure.figsize\": (8, 6)}, font=\"monospace\", context=\"notebook\", color_codes=True)\n",
    "\n",
    "\n",
    "try:\n",
    "    from entmax import entmax_bisect\n",
    "    print(\"Successfully imported entmax_bisect.\")\n",
    "    ENTMAX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: entmax library not found or import failed.\")\n",
    "    print(\"         AlphaEntmaxRouter will default to Softmax for alpha != 1.0.\")\n",
    "    print(\"         Install entmax: pip install entmax\")\n",
    "    ENTMAX_AVAILABLE = False\n",
    "    def entmax_bisect(*args, **kwargs):\n",
    "        print(\"Fallback: Using Softmax instead of entmax_bisect (for alpha != 1.0).\")\n",
    "        logits = args[0]\n",
    "        return F.softmax(logits, dim=-1)\n",
    "\n",
    "# --- Base Configuration () ---\n",
    "config_base = {\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"dropout\": 0.1,\n",
    "    \"eval_iters_scale_factor\": 10,\n",
    "    \"router_alpha\": 1.5,\n",
    "    \"router_temperature\": 1.0,\n",
    "    \"entmax_n_iter\": 25,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# --- Sweep Configuration () ---\n",
    "config_sweep = {\n",
    "    **config_base,\n",
    "    \"batch_size\": 64,\n",
    "    \"block_size\": 64,\n",
    "    \"max_iters\": 500,\n",
    "    \"eval_interval\": 20,\n",
    "    \"n_embed\": 64,\n",
    "    \"n_head\": 4,\n",
    "    \"n_layer\": 3,\n",
    "    \"num_experts\": 4,\n",
    "}\n",
    "# --- Full Run Configuration ---\n",
    "config_full = {\n",
    "    **config_base,\n",
    "    \"batch_size\": 128,\n",
    "    \"block_size\": 128,\n",
    "    \"max_iters\": 5000,\n",
    "    \"eval_interval\": 100,\n",
    "    \"n_embed\": 128,\n",
    "    \"n_head\": 8,\n",
    "    \"n_layer\": 6,\n",
    "    \"num_experts\": 8,\n",
    "    \"router_alpha\": config_base[\"router_alpha\"],\n",
    "    \"router_temperature\": config_base[\"router_temperature\"],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "config_full[\"eval_iters\"] = (config_full[\"max_iters\"] // config_full[\"eval_interval\"]) * config_full[\"eval_iters_scale_factor\"]\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Data Loading and Preprocessing (Tiny Shakespeare) ---\n",
    "data_file = 'input.txt'\n",
    "\n",
    "if not os.path.exists(data_file):\n",
    "    print(f\"Error: Dataset file '{data_file}' not found!\")\n",
    "    print(f\"Please download Tiny Shakespeare input.txt and place it here.\")\n",
    "    print(f\"E.g., from: https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    print(f\"Successfully loaded {data_file}\")\n",
    "\n",
    "    chars = sorted(list(set(text)))\n",
    "    full_vocab_size = len(chars)\n",
    "    stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "    itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "    encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
    "    decode = lambda l: ''.join([itos[i] for i in l if i in itos])\n",
    "\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    print(f\"Dataset Stats: Vocab size: {full_vocab_size}, Total tokens: {len(data)}\")\n",
    "\n",
    "    # Split for SWEEP runs (using 90% of full data) - as defined in notebook\n",
    "    n_split = int(0.9 * len(data))\n",
    "    train_data = data[:n_split] # This will be used by the sweep\n",
    "    val_data = data[n_split:]   # This will be used by the sweep\n",
    "    print(f\"Sweep Data Split: Train tokens: {len(train_data)}, Valid tokens: {len(val_data)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error processing dataset file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Model Definitions  ---\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One head of self-attention \"\"\"\n",
    "    def __init__(self, n_embed, head_size, block_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.head_size = head_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * self.head_size**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-head self-attention module \"\"\"\n",
    "    def __init__(self, n_embed, num_heads, block_size, dropout):\n",
    "        super().__init__()\n",
    "        assert n_embed % num_heads == 0\n",
    "        head_size = n_embed // num_heads\n",
    "        self.heads = nn.ModuleList([Head(n_embed, head_size, block_size, dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\" Simple FeedForward Expert network \"\"\"\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class AlphaEntmaxRouter(nn.Module):\n",
    "    \"\"\" Router using alpha-entmax, with explicit Softmax for alpha=1.0 \"\"\"\n",
    "    def __init__(self, n_embed, num_experts, alpha=1.5, temperature=1.0, n_iter=25):\n",
    "        super().__init__()\n",
    "        assert temperature > 1e-9\n",
    "        self.num_experts = num_experts\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        self.n_iter = n_iter\n",
    "        self.route_linear = nn.Linear(n_embed, num_experts)\n",
    "        if not ENTMAX_AVAILABLE and self.alpha != 1.0:\n",
    "             print(f\"Warning: entmax library not found, alpha={self.alpha} runs will use Softmax fallback.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.route_linear(x)\n",
    "        scaled_logits = logits / self.temperature\n",
    "        router_output = None\n",
    "\n",
    "        if self.alpha == 1.0:\n",
    "            router_output = F.softmax(scaled_logits, dim=-1)\n",
    "        elif ENTMAX_AVAILABLE:\n",
    "            try:\n",
    "                router_output = entmax_bisect(scaled_logits, alpha=self.alpha, dim=-1, n_iter=self.n_iter)\n",
    "            except Exception as e:\n",
    "                print(f\"WARNING: entmax_bisect failed with alpha={self.alpha}. Error: {e}. Falling back to Softmax.\")\n",
    "                router_output = F.softmax(scaled_logits, dim=-1)\n",
    "        else:\n",
    "             router_output = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "        if router_output is None:\n",
    "             print(\"ERROR: router_output logic failed. Defaulting to Softmax.\")\n",
    "             router_output = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "        return router_output\n",
    "\n",
    "class SparseMoE(nn.Module):\n",
    "    \"\"\" Sparse Mixture of Experts layer \"\"\"\n",
    "    def __init__(self, n_embed, num_experts, dropout, router_alpha=1.5, router_temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.router = AlphaEntmaxRouter(n_embed, num_experts, alpha=router_alpha, temperature=router_temperature)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed, dropout) for _ in range(num_experts)])\n",
    "        self.num_experts = num_experts\n",
    "        self.register_buffer('latest_tpe', torch.zeros(num_experts, dtype=torch.float32))\n",
    "        self.latest_imbalance: float = 1.0\n",
    "        self.latest_concentration: float = 0.0\n",
    "        self.latest_utilization: float = 0.0\n",
    "        self.latest_tpe_cv: float = 0.0\n",
    "        self.latest_avg_prob_cv: float = 0.0\n",
    "        self.activation_threshold = 1e-9\n",
    "\n",
    "    def calculate_metrics(self, gating_output_no_grad):\n",
    "        \"\"\" Calculates routing metrics based on detached gating output \"\"\"\n",
    "        num_tokens = gating_output_no_grad.shape[0]\n",
    "        if num_tokens == 0 or self.num_experts == 0:\n",
    "            self.latest_tpe.zero_()\n",
    "            self.latest_imbalance = 1.0\n",
    "            self.latest_concentration = 0.0\n",
    "            self.latest_utilization = 0.0\n",
    "            self.latest_tpe_cv = 0.0\n",
    "            self.latest_avg_prob_cv = 0.0\n",
    "            return\n",
    "\n",
    "        with torch.no_grad():\n",
    "            is_active = gating_output_no_grad > self.activation_threshold\n",
    "            tpe = is_active.sum(dim=0).float()\n",
    "            if self.latest_tpe.shape[0] == tpe.shape[0]:\n",
    "                 self.latest_tpe.copy_(tpe)\n",
    "            else:\n",
    "                 print(f\"Warning: TPE size mismatch. Re-registering buffer.\")\n",
    "                 self.register_buffer('latest_tpe', tpe.clone())\n",
    "\n",
    "            if self.num_experts > 1:\n",
    "                mean_tpe = tpe.mean()\n",
    "                std_tpe = tpe.std(unbiased=False)\n",
    "                self.latest_tpe_cv = (std_tpe / (mean_tpe + 1e-9)).item() if mean_tpe > 1e-9 else 0.0\n",
    "            else:\n",
    "                self.latest_tpe_cv = 0.0\n",
    "\n",
    "            mean_tpe_val = tpe.mean().item()\n",
    "            if mean_tpe_val > 0:\n",
    "                 self.latest_imbalance = tpe.max().item() / (mean_tpe_val + 1e-9)\n",
    "            else:\n",
    "                 self.latest_imbalance = 1.0\n",
    "\n",
    "            max_p_per_token, _ = gating_output_no_grad.max(dim=-1)\n",
    "            self.latest_concentration = max_p_per_token.mean().item()\n",
    "\n",
    "            num_active_experts = (tpe > 0).sum().item()\n",
    "            self.latest_utilization = num_active_experts / self.num_experts if self.num_experts > 0 else 0.0\n",
    "\n",
    "            if self.num_experts > 1:\n",
    "                avg_prob = gating_output_no_grad.mean(dim=0)\n",
    "                mean_avg_prob = avg_prob.mean()\n",
    "                std_avg_prob = avg_prob.std(unbiased=False)\n",
    "                self.latest_avg_prob_cv = (std_avg_prob / (mean_avg_prob + 1e-9)).item() if mean_avg_prob > 1e-9 else 0.0\n",
    "            else:\n",
    "                self.latest_avg_prob_cv = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "        x_reshaped = x.view(num_tokens, n_embed)\n",
    "        gating_output = self.router(x_reshaped)\n",
    "        self.calculate_metrics(gating_output.detach())\n",
    "        final_output = torch.zeros_like(x_reshaped)\n",
    "        for i, expert in enumerate(self.experts):\n",
    "             token_indices = torch.nonzero(gating_output[:, i] > self.activation_threshold).squeeze(-1)\n",
    "             if token_indices.numel() == 0: continue\n",
    "             expert_input = x_reshaped[token_indices]\n",
    "             active_gating_scores = gating_output[token_indices, i].unsqueeze(1)\n",
    "             expert_output = expert(expert_input)\n",
    "             weighted_output = expert_output * active_gating_scores\n",
    "             final_output.index_add_(0, token_indices, weighted_output)\n",
    "        final_output = final_output.view(batch_size, seq_len, n_embed)\n",
    "        return final_output\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation (MoE) \"\"\"\n",
    "    def __init__(self, n_embed, n_head, num_experts, block_size, dropout, router_alpha=1.5, router_temperature=1.0):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        self.self_attn = MultiHeadAttention(n_embed, n_head, block_size, dropout)\n",
    "        self.sparse_moe = SparseMoE(n_embed, num_experts, dropout, router_alpha=router_alpha, router_temperature=router_temperature)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attn(self.ln1(x))\n",
    "        x = x + self.sparse_moe(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class SparseMoELanguageModel(nn.Module):\n",
    "    \"\"\" Language Model using SparseMoE Blocks \"\"\"\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_layer, num_experts, block_size, dropout, router_alpha=1.5, router_temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.block_size = block_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(n_embed=n_embed, n_head=n_head, num_experts=num_experts, block_size=block_size, dropout=dropout,\n",
    "                  router_alpha=router_alpha, router_temperature=router_temperature)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        device = idx.device\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos = torch.arange(T, device=device)\n",
    "\n",
    "        if T > self.block_size:\n",
    "             print(f\"Warning: Input sequence length T={T} exceeds block_size={self.block_size}. Truncating.\")\n",
    "             pos = pos[-self.block_size:]\n",
    "             tok_emb = tok_emb[:, -self.block_size:, :]\n",
    "             T = self.block_size\n",
    "\n",
    "        pos_emb = self.position_embedding_table(pos)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B_logits, T_logits, C_logits = logits.shape\n",
    "            if targets.shape[1] > T_logits:\n",
    "                 targets = targets[:, -T_logits:]\n",
    "\n",
    "            logits_flat = logits.view(B_logits * T_logits, C_logits)\n",
    "            targets_flat = targets.view(B_logits * T_logits)\n",
    "            loss = F.cross_entropy(logits_flat, targets_flat)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "# --- Utility Functions ---\n",
    "\n",
    "def get_batch(split, data_train, data_val, block_size, batch_size, device):\n",
    "    \"\"\" Generate a small batch of data of inputs x and targets y \"\"\"\n",
    "    data_source = data_train if split == 'train' else data_val\n",
    "    max_start_index = len(data_source) - block_size -1\n",
    "    if max_start_index < 0:\n",
    "         print(f\"Warning: Dataset split length ({len(data_source)}) is smaller than block_size ({block_size}). Cannot generate batch.\")\n",
    "         return None, None\n",
    "\n",
    "    ix = torch.randint(max_start_index + 1, (batch_size,))\n",
    "    x = torch.stack([data_source[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data_source[i + 1 : i + block_size + 1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, config, data_train, data_val):\n",
    "    \"\"\" Estimate loss and metrics on train/val splits \"\"\"\n",
    "    out = {'loss': {}}\n",
    "    eval_iters = config[\"eval_iters\"]\n",
    "    block_size = config[\"block_size\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    n_layer = config[\"n_layer\"]\n",
    "    num_experts = config[\"num_experts\"]\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    val_metrics_accumulated = {\n",
    "        f'L{layer_idx}': {'imbalance': 0.0, 'concentration': 0.0, 'utilization': 0.0,\n",
    "                           'tpe_cv': 0.0, 'avg_prob_cv': 0.0, 'count': 0}\n",
    "        for layer_idx in range(n_layer)\n",
    "    }\n",
    "\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        if split == 'val':\n",
    "            for layer_idx in range(n_layer):\n",
    "                layer_metrics = val_metrics_accumulated[f'L{layer_idx}']\n",
    "                for key in layer_metrics:\n",
    "                    layer_metrics[key] = 0.0 if key != 'count' else 0\n",
    "        for k in range(eval_iters):\n",
    "            xb, yb = get_batch(split, data_train, data_val, block_size, batch_size, device)\n",
    "            if xb is None or yb is None:\n",
    "                losses[k] = float('nan')\n",
    "                continue\n",
    "\n",
    "            logits, loss = model(xb, yb)\n",
    "            losses[k] = loss.item() if loss is not None else float('nan')\n",
    "\n",
    "            if split == 'val' and hasattr(model, 'blocks') and isinstance(model.blocks, nn.Sequential):\n",
    "                for i, block in enumerate(model.blocks):\n",
    "                     if hasattr(block, 'sparse_moe') and isinstance(block.sparse_moe, SparseMoE):\n",
    "                         smoe_layer = block.sparse_moe\n",
    "                         layer_key = f'L{i}'\n",
    "                         layer_metrics = val_metrics_accumulated[layer_key]\n",
    "                         if hasattr(smoe_layer, 'latest_imbalance'):\n",
    "                             # Accumulate metrics only if they are valid numbers\n",
    "                             if not np.isnan(smoe_layer.latest_imbalance): layer_metrics['imbalance'] += smoe_layer.latest_imbalance\n",
    "                             if not np.isnan(smoe_layer.latest_concentration): layer_metrics['concentration'] += smoe_layer.latest_concentration\n",
    "                             if not np.isnan(smoe_layer.latest_utilization): layer_metrics['utilization'] += smoe_layer.latest_utilization\n",
    "                             if not np.isnan(smoe_layer.latest_tpe_cv): layer_metrics['tpe_cv'] += smoe_layer.latest_tpe_cv\n",
    "                             if not np.isnan(smoe_layer.latest_avg_prob_cv): layer_metrics['avg_prob_cv'] += smoe_layer.latest_avg_prob_cv\n",
    "                             layer_metrics['count'] += 1\n",
    "\n",
    "        out['loss'][split] = np.nanmean(losses.numpy()) if not torch.all(torch.isnan(losses)) else float('nan')\n",
    "\n",
    "    total_metrics_sum = {'imbalance': 0.0, 'concentration': 0.0, 'utilization': 0.0,\n",
    "                         'tpe_cv': 0.0, 'avg_prob_cv': 0.0}\n",
    "    total_layers_with_metrics = 0\n",
    "\n",
    "    for i in range(n_layer):\n",
    "        count = val_metrics_accumulated[f'L{i}']['count']\n",
    "        if count > 0:\n",
    "            total_layers_with_metrics += 1\n",
    "            metrics_sum = val_metrics_accumulated[f'L{i}']\n",
    "            for key in total_metrics_sum:\n",
    "                 metric_value = metrics_sum[key]\n",
    "                 if not np.isnan(metric_value):\n",
    "                     avg_layer_metric = metric_value / count\n",
    "                     total_metrics_sum[key] += avg_layer_metric\n",
    "\n",
    "    overall_avg_metrics = {}\n",
    "    if total_layers_with_metrics > 0:\n",
    "        for key in total_metrics_sum:\n",
    "            overall_avg_metrics[key] = total_metrics_sum[key] / total_layers_with_metrics\n",
    "    else:\n",
    "        for key in total_metrics_sum:\n",
    "             overall_avg_metrics[key] = 0.0\n",
    "\n",
    "    model.train()\n",
    "    return out['loss'], overall_avg_metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a68df14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Hyperparameter Sweep ---\n",
      "Sweep Config: {'learning_rate': 0.001, 'dropout': 0.1, 'eval_iters_scale_factor': 10, 'router_alpha': 1.5, 'router_temperature': 1.0, 'entmax_n_iter': 25, 'seed': 42, 'batch_size': 128, 'block_size': 128, 'max_iters': 5000, 'eval_interval': 100, 'n_embed': 128, 'n_head': 8, 'n_layer': 6, 'num_experts': 8, 'eval_iters': 500}\n",
      "Sweeping Alphas: [1.0, 1.5, 2.0, 2.5]\n",
      "Sweeping Temperatures: [0.5, 1.0, 10.0]\n",
      "Running 5 seeds per combination.\n",
      "\n",
      "----- Starting Sweep Run: alpha_1.0_temp_0.5_seed_42 -----\n",
      "Setting seed: 42\n",
      "Sweep Model Params: 6.76 M\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5cdf3849cd5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0miter_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_iters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0miter_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_interval\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miter_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"max_iters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_metrics_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msweep_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msweep_val_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                     \u001b[0mloss_train_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0mloss_val_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AGT/moe_venv/lib64/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1600d5588db0>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m(model, config, data_train, data_val)\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AGT/moe_venv/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1600d5588db0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mtok_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AGT/moe_venv/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AGT/moe_venv/lib64/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AGT/moe_venv/lib64/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# --- SECTION 1: Hyperparameter Sweep ---\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n--- Running Hyperparameter Sweep ---\")\n",
    "\n",
    "\n",
    "alphas = [1.0, 1.5, 2.0, 2.5]\n",
    "temperatures = [0.5, 1.0, 10.0]\n",
    "num_seeds = 5\n",
    "base_seed = config_base[\"seed\"]\n",
    "config = config_full\n",
    "\n",
    "print(f\"Sweep Config: {config}\")\n",
    "print(f\"Sweeping Alphas: {alphas}\")\n",
    "print(f\"Sweeping Temperatures: {temperatures}\")\n",
    "print(f\"Running {num_seeds} seeds per combination.\")\n",
    "\n",
    "\n",
    "sweep_train_data = train_data\n",
    "sweep_val_data = val_data\n",
    "\n",
    "dtypes = {'model_key': str, 'alpha': float, 'temperature': float, 'seed': int, 'iter': int,\n",
    "          'loss_train': float, 'loss_val': float,\n",
    "          'Expert Utilization': float, 'Load Imbalance Ratio': float, 'Routing Concentration': float,\n",
    "          'Expert Load Variation': float, 'Routing Probability CV': float}\n",
    "df_sweep_results = pd.DataFrame({col: pd.Series(dtype=dt) for col, dt in dtypes.items()})\n",
    "\n",
    "sweep_start_time = time.time()\n",
    "\n",
    "# --- Sweep Loops (with Multi-Seed) ---\n",
    "for alpha in alphas:\n",
    "    for temperature in temperatures:\n",
    "        for seed_run_index in range(num_seeds): # Iterate through seeds\n",
    "            current_seed = base_seed + seed_run_index\n",
    "            run_key = f\"alpha_{alpha}_temp_{temperature}_seed_{current_seed}\"\n",
    "            print(f\"\\n----- Starting Sweep Run: {run_key} -----\")\n",
    "\n",
    "            print(f\"Setting seed: {current_seed}\")\n",
    "            torch.manual_seed(current_seed)\n",
    "            np.random.seed(current_seed) # Seed numpy as well\n",
    "\n",
    "            config['router_alpha'] = alpha\n",
    "            config['router_temperature'] = temperature\n",
    "\n",
    "            # --- Re-initialize Model and Optimizer for each seed ---\n",
    "            model = SparseMoELanguageModel(\n",
    "                vocab_size=full_vocab_size,\n",
    "                n_embed=config[\"n_embed\"], n_head=config[\"n_head\"], n_layer=config[\"n_layer\"],\n",
    "                num_experts=config[\"num_experts\"], block_size=config[\"block_size\"], dropout=config[\"dropout\"],\n",
    "                router_alpha=config[\"router_alpha\"], router_temperature=config[\"router_temperature\"]\n",
    "            ).to(device)\n",
    "            print(f\"Sweep Model Params: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M\")\n",
    "\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "            run_start_time = time.time()\n",
    "            # --- Training Loop ---\n",
    "            for iter_num in range(config[\"max_iters\"]):\n",
    "                if iter_num % config[\"eval_interval\"] == 0 or iter_num == config[\"max_iters\"] - 1:\n",
    "                    losses, avg_metrics_val = estimate_loss(model, config, sweep_train_data, sweep_val_data)\n",
    "                    loss_train_val = losses.get('train', float('nan'))\n",
    "                    loss_val_val = losses.get('val', float('nan'))\n",
    "\n",
    "                    new_row_data = {\n",
    "                        'model_key': f\"alpha_{alpha}_temp_{temperature}\",\n",
    "                        'alpha': alpha, 'temperature': temperature, 'seed': current_seed, 'iter': iter_num,\n",
    "                        'loss_train': loss_train_val, 'loss_val': loss_val_val,\n",
    "                        'Expert Utilization': avg_metrics_val.get('utilization', 0.0),\n",
    "                        'Load Imbalance Ratio': avg_metrics_val.get('imbalance', 0.0),\n",
    "                        'Routing Concentration': avg_metrics_val.get('concentration', 0.0),\n",
    "                        'Expert Load Variation': avg_metrics_val.get('tpe_cv', 0.0),\n",
    "                        'Routing Probability CV': avg_metrics_val.get('avg_prob_cv', 0.0)\n",
    "                    }\n",
    "                    new_row = pd.DataFrame([new_row_data])\n",
    "                    # --- Append results safely (Handles FutureWarning) ---\n",
    "                    if not new_row.empty and not new_row.isnull().all().all():\n",
    "                        df_sweep_results = pd.concat([df_sweep_results, new_row], ignore_index=True)\n",
    "                    # --- ---\n",
    "\n",
    "                    # Use exact print format requested\n",
    "                    print(f\"Sweep Run: {run_key} | Iter: {iter_num:<4}/{config['max_iters']:<4} || Est Train Loss: {loss_train_val:.4f} | Val Loss: {loss_val_val:.4f}\")\n",
    "                    print(f\"Metrics (Avg) || Expert Utilization: {avg_metrics_val.get('utilization', 0.0):<6.2f} | Load Imbalance Ratio: {avg_metrics_val.get('imbalance', 0.0):<6.2f} | Routing Concentration: {avg_metrics_val.get('concentration', 0.0):<6.2f} | Expert Load Variation: {avg_metrics_val.get('tpe_cv', 0.0):<6.2f} | Routing Probability CV: {avg_metrics_val.get('avg_prob_cv', 0.0):<6.2f}\")\n",
    "\n",
    "                xb, yb = get_batch('train', sweep_train_data, sweep_val_data, config[\"block_size\"], config[\"batch_size\"], device)\n",
    "                if xb is None or yb is None: continue # Skip if batch failed\n",
    "                logits, loss = model(xb, yb)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            df_sweep_results.to_csv('output_results.csv')\n",
    "\n",
    "            run_end_time = time.time()\n",
    "            print(f\"----- Sweep Run {run_key} completed in {run_end_time - run_start_time:.2f} seconds -----\")\n",
    "\n",
    "sweep_end_time = time.time()\n",
    "print(f\"\\n--- Full Sweep (all seeds) completed in {sweep_end_time - sweep_start_time:.2f} seconds ---\")\n",
    "\n",
    "sweep_results_file = 'sweep_results_all_seeds.csv'\n",
    "print(f\"\\nSaving detailed sweep results to {sweep_results_file}\")\n",
    "df_sweep_results.to_csv(sweep_results_file, index=False)\n",
    "\n",
    "print(\"\\n--- Aggregated Sweep Results (Averaged Across Seeds) ---\")\n",
    "final_iter_sweep = config_sweep[\"max_iters\"] - 1\n",
    "df_final_iter_sweep = df_sweep_results[df_sweep_results['iter'] == final_iter_sweep].copy()\n",
    "\n",
    "metric_cols_display = ['loss_val', 'Expert Utilization', 'Load Imbalance Ratio',\n",
    "                       'Routing Concentration', 'Expert Load Variation', 'Routing Probability CV']\n",
    "\n",
    "best_sweep_params = None\n",
    "if not df_final_iter_sweep.empty:\n",
    "    grouped_sweep_results = df_final_iter_sweep.groupby(['alpha', 'temperature'])\n",
    "    aggregated_sweep_stats = grouped_sweep_results[metric_cols_display].agg(['mean', 'std'])\n",
    "    aggregated_sweep_stats.columns = ['_'.join(col).strip() for col in aggregated_sweep_stats.columns.values]\n",
    "    aggregated_sweep_stats = aggregated_sweep_stats.sort_values('loss_val_mean')\n",
    "\n",
    "    print(aggregated_sweep_stats)\n",
    "    agg_sweep_results_file = 'sweep_results_aggregated.csv'\n",
    "    print(f\"\\nSaving aggregated sweep results to {agg_sweep_results_file}\")\n",
    "    aggregated_sweep_stats.to_csv(agg_sweep_results_file)\n",
    "\n",
    "    best_params_idx = aggregated_sweep_stats['loss_val_mean'].idxmin()\n",
    "    best_sweep_params = {'alpha': best_params_idx[0], 'temperature': best_params_idx[1]}\n",
    "    print(f\"\\nBest parameters found from sweep: Alpha={best_sweep_params['alpha']}, Temperature={best_sweep_params['temperature']}\")\n",
    "else:\n",
    "    print(\"No final iteration results found from sweep to aggregate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed59b5cabb480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SECTION 2: Full Dataset Run ---\n",
    "# ============================================================\n",
    "\n",
    "print(f\"\\n--- Running Full Dataset Training ---\")\n",
    "\n",
    "config = config_full\n",
    "num_seeds_full = 3\n",
    "\n",
    "print(f\"Full Run Config: {config}\")\n",
    "print(f\"Running {num_seeds_full} seeds.\")\n",
    "\n",
    "n_full = int(0.9 * len(data))\n",
    "train_data_full = data[:n_full]\n",
    "val_data_full = data[n_full:]\n",
    "print(f\"Full Data Split: Train tokens: {len(train_data_full)}, Valid tokens: {len(val_data_full)}\")\n",
    "\n",
    "# Use the best parameters from sweep if found, otherwise use config_full defaults\n",
    "if best_sweep_params:\n",
    "    print(f\"Using best parameters from sweep: Alpha={best_sweep_params['alpha']}, Temp={best_sweep_params['temperature']}\")\n",
    "    config['router_alpha'] = best_sweep_params['alpha']\n",
    "    config['router_temperature'] = best_sweep_params['temperature']\n",
    "else:\n",
    "    print(f\"Using default parameters from config_full: Alpha={config['router_alpha']}, Temp={config['router_temperature']}\")\n",
    "\n",
    "dtypes_full = {'seed': int, 'iter': int, 'loss_train': float, 'loss_val': float,\n",
    "               'Expert Utilization': float, 'Load Imbalance Ratio': float, 'Routing Concentration': float,\n",
    "               'Expert Load Variation': float, 'Routing Probability CV': float}\n",
    "df_full_run_results = pd.DataFrame({col: pd.Series(dtype=dt) for col, dt in dtypes_full.items()})\n",
    "\n",
    "full_run_start_time = time.time()\n",
    "\n",
    "# --- Full Run Seed Loop ---\n",
    "for seed_run_index in range(num_seeds_full):\n",
    "    current_seed = base_seed + seed_run_index # Using same base seed sequence\n",
    "    run_key = f\"full_run_alpha_{config['router_alpha']}_temp_{config['router_temperature']}_seed_{current_seed}\"\n",
    "    print(f\"\\n----- Starting Full Run: {run_key} -----\")\n",
    "\n",
    "    print(f\"Setting seed: {current_seed}\")\n",
    "    torch.manual_seed(current_seed)\n",
    "    np.random.seed(current_seed)\n",
    "    model_full = SparseMoELanguageModel(\n",
    "        vocab_size=full_vocab_size,\n",
    "        n_embed=config[\"n_embed\"], n_head=config[\"n_head\"], n_layer=config[\"n_layer\"],\n",
    "        num_experts=config[\"num_experts\"], block_size=config[\"block_size\"], dropout=config[\"dropout\"],\n",
    "        router_alpha=config[\"router_alpha\"], router_temperature=config[\"router_temperature\"]\n",
    "    ).to(device)\n",
    "    print(f\"Full Model Params: {sum(p.numel() for p in model_full.parameters()) / 1e6:.2f} M\")\n",
    "\n",
    "    optimizer_full = torch.optim.AdamW(model_full.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    run_start_time = time.time()\n",
    "    # --- Full Training Loop ---\n",
    "    for iter_num in range(config[\"max_iters\"]):\n",
    "        if iter_num % config[\"eval_interval\"] == 0 or iter_num == config[\"max_iters\"] - 1:\n",
    "            losses, avg_metrics_val = estimate_loss(model_full, config, train_data_full, val_data_full)\n",
    "            loss_train_val = losses.get('train', float('nan'))\n",
    "            loss_val_val = losses.get('val', float('nan'))\n",
    "\n",
    "            new_row_data = {\n",
    "                'seed': current_seed, 'iter': iter_num,\n",
    "                'loss_train': loss_train_val, 'loss_val': loss_val_val,\n",
    "                'Expert Utilization': avg_metrics_val.get('utilization', 0.0),\n",
    "                'Load Imbalance Ratio': avg_metrics_val.get('imbalance', 0.0),\n",
    "                'Routing Concentration': avg_metrics_val.get('concentration', 0.0),\n",
    "                'Expert Load Variation': avg_metrics_val.get('tpe_cv', 0.0),\n",
    "                'Routing Probability CV': avg_metrics_val.get('avg_prob_cv', 0.0)\n",
    "            }\n",
    "            new_row = pd.DataFrame([new_row_data])\n",
    "            # --- Append results safely ---\n",
    "            if not new_row.empty and not new_row.isnull().all().all():\n",
    "                df_full_run_results = pd.concat([df_full_run_results, new_row], ignore_index=True)\n",
    "            # --- ---\n",
    "\n",
    "            # Use exact print format requested\n",
    "            print(f\"Full Run: {run_key} | Iter: {iter_num:<5}/{config['max_iters']:<5} || Est Train Loss: {loss_train_val:.4f} | Val Loss: {loss_val_val:.4f}\")\n",
    "            print(f\"Metrics (Avg) || Expert Utilization: {avg_metrics_val.get('utilization', 0.0):<6.2f} | Load Imbalance Ratio: {avg_metrics_val.get('imbalance', 0.0):<6.2f} | Routing Concentration: {avg_metrics_val.get('concentration', 0.0):<6.2f} | Expert Load Variation: {avg_metrics_val.get('tpe_cv', 0.0):<6.2f} | Routing Probability CV: {avg_metrics_val.get('avg_prob_cv', 0.0):<6.2f}\")\n",
    "\n",
    "        # Use the full data splits for training batch\n",
    "        xb, yb = get_batch('train', train_data_full, val_data_full, config[\"block_size\"], config[\"batch_size\"], device)\n",
    "        if xb is None or yb is None: continue\n",
    "        logits, loss = model_full(xb, yb)\n",
    "        optimizer_full.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer_full.step()\n",
    "\n",
    "    run_end_time = time.time()\n",
    "    print(f\"----- Full Run {run_key} completed in {run_end_time - run_start_time:.2f} seconds -----\")\n",
    "\n",
    "full_run_overall_end_time = time.time()\n",
    "\n",
    "print(f\"\\n--- Full Dataset Training (all seeds) completed in {full_run_overall_end_time - full_run_start_time:.2f} seconds ---\")\n",
    "\n",
    "full_run_results_file = 'full_run_results_all_seeds.csv'\n",
    "print(f\"\\nSaving detailed full run results to {full_run_results_file}\")\n",
    "df_full_run_results.to_csv(full_run_results_file, index=False)\n",
    "\n",
    "print(\"\\n--- Aggregated Full Run Results (Averaged Across Seeds) ---\")\n",
    "final_iter_full = config_full[\"max_iters\"] - 1\n",
    "df_final_iter_full = df_full_run_results[df_full_run_results['iter'] == final_iter_full].copy()\n",
    "\n",
    "if not df_final_iter_full.empty:\n",
    "    aggregated_full_stats = df_final_iter_full[metric_cols_display + ['loss_train']].agg(['mean', 'std'])\n",
    "    print(aggregated_full_stats)\n",
    "\n",
    "    agg_full_results_file = 'full_run_results_aggregated.csv'\n",
    "    print(f\"\\nSaving aggregated full run results to {agg_full_results_file}\")\n",
    "    aggregated_full_stats.to_csv(agg_full_results_file)\n",
    "else:\n",
    "    print(\"No final iteration results found from full run to aggregate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c833d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()\n",
    "df.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a947c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n",
      "1.10.2+cu102\n",
      "10.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "217479d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  3 03:01:57 2025       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:86:00.0 Off |                    0 |\r\n",
      "| N/A   33C    P0              34W / 250W |    475MiB / 40960MiB |      0%      Default |\r\n",
      "|                                         |                      |             Disabled |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A   2248931      C   /home/higoyal/AGT/moe_venv/bin/python       466MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb9271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
